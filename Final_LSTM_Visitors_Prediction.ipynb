{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqa0hsJ7rzsE",
        "outputId": "ceb1f701-2358-4abb-c398-1f030c7bd0d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOAeWZPIsIpy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Team7/ForwardKeys_data.csv')\n",
        "\n",
        "# date to datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "\n",
        "# one hot encodings for day\n",
        "dayofweek_onehot = pd.get_dummies(df['DayOfWeek'], prefix='Day')\n",
        "df = df.join(dayofweek_onehot)\n",
        "\n",
        "# all location\n",
        "locations = ['Visitors in Blue Lagoon', 'Visitors in Machu Picchu', 'Visitors in Taj Mahal', 'Visitors in Doge\\'s Palace', 'Visitors in Louvre Museum']\n",
        "\n",
        "# hold seq data\n",
        "sequence_data_all = {}\n",
        "\n",
        "# normalize\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "for location in locations:\n",
        "    for time_slot in df['Time'].unique():\n",
        "\n",
        "        # filter data for current loc and timeslot\n",
        "        df_filtered = df[df['Time'] == time_slot][['Date', location] + dayofweek_onehot.columns.tolist()].copy()\n",
        "        df_filtered[location] = scaler.fit_transform(df_filtered[[location]])\n",
        "\n",
        "        # make data\n",
        "        sequence_data = df_filtered[[location] + dayofweek_onehot.columns.tolist()].astype(np.float32)\n",
        "\n",
        "        # create sequences\n",
        "        def create_sequences(data, seq_length):\n",
        "            xs, ys = [], []\n",
        "            for i in range(len(data) - seq_length):\n",
        "                x = data.iloc[i:(i + seq_length)].values\n",
        "                y = data.iloc[i + seq_length, 0]\n",
        "                xs.append(x)\n",
        "                ys.append(y)\n",
        "            return np.array(xs), np.array(ys)\n",
        "\n",
        "        seq_length = 50\n",
        "        X, y = create_sequences(sequence_data, seq_length)\n",
        "\n",
        "        # convert PyTorch tensors\n",
        "        X_tensor = torch.from_numpy(X)\n",
        "        y_tensor = torch.from_numpy(y).view(-1, 1)\n",
        "        sequence_data_all[(location, time_slot)] = (X_tensor, y_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l2Zer4_wI2k",
        "outputId": "9e238762-f3df-44f7-db04-bc365b7dda96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM - Epoch 0, Loss: 0.19154991209506989 at 8:00\n",
            "LSTM - Epoch 10, Loss: 0.11313506960868835 at 8:00\n",
            "LSTM - Epoch 20, Loss: 0.10728836059570312 at 8:00\n",
            "LSTM - Epoch 30, Loss: 0.10543463379144669 at 8:00\n",
            "LSTM - Epoch 40, Loss: 0.1041603833436966 at 8:00\n",
            "LSTM - Epoch 50, Loss: 0.10086940973997116 at 8:00\n",
            "LSTM - Epoch 60, Loss: 0.0906166285276413 at 8:00\n",
            "LSTM - Epoch 70, Loss: 0.049151461571455 at 8:00\n",
            "LSTM - Epoch 80, Loss: 0.03175458684563637 at 8:00\n",
            "LSTM - Epoch 90, Loss: 0.02410230040550232 at 8:00\n",
            "LSTM - Epoch 100, Loss: 0.01693207398056984 at 8:00\n",
            "LSTM - Epoch 110, Loss: 0.013494571670889854 at 8:00\n",
            "LSTM - Epoch 120, Loss: 0.012184220366179943 at 8:00\n",
            "LSTM - Epoch 130, Loss: 0.011714310385286808 at 8:00\n",
            "LSTM - Epoch 140, Loss: 0.011515283025801182 at 8:00\n",
            "LSTM - Epoch 150, Loss: 0.01135043241083622 at 8:00\n",
            "LSTM - Epoch 160, Loss: 0.011230788193643093 at 8:00\n",
            "Time Slot: 8:00, LSTM R-squared: 0.8815473650209716, RMSE: 0.10950367152690887\n",
            "LSTM - Epoch 0, Loss: 0.29247206449508667 at 10:00\n",
            "LSTM - Epoch 10, Loss: 0.10912858694791794 at 10:00\n",
            "LSTM - Epoch 20, Loss: 0.1051030158996582 at 10:00\n",
            "LSTM - Epoch 30, Loss: 0.10484058409929276 at 10:00\n",
            "LSTM - Epoch 40, Loss: 0.1037730872631073 at 10:00\n",
            "LSTM - Epoch 50, Loss: 0.10183005779981613 at 10:00\n",
            "LSTM - Epoch 60, Loss: 0.09881645441055298 at 10:00\n",
            "LSTM - Epoch 70, Loss: 0.09190439432859421 at 10:00\n",
            "LSTM - Epoch 80, Loss: 0.06991159170866013 at 10:00\n",
            "LSTM - Epoch 90, Loss: 0.03452644497156143 at 10:00\n",
            "LSTM - Epoch 100, Loss: 0.024190640076994896 at 10:00\n",
            "LSTM - Epoch 110, Loss: 0.018798381090164185 at 10:00\n",
            "LSTM - Epoch 120, Loss: 0.014541606418788433 at 10:00\n",
            "LSTM - Epoch 130, Loss: 0.012740745209157467 at 10:00\n",
            "LSTM - Epoch 140, Loss: 0.01222915668040514 at 10:00\n",
            "LSTM - Epoch 150, Loss: 0.01207031961530447 at 10:00\n",
            "LSTM - Epoch 160, Loss: 0.011919434182345867 at 10:00\n",
            "Time Slot: 10:00, LSTM R-squared: 0.8814921402193426, RMSE: 0.11191629618406296\n",
            "LSTM - Epoch 0, Loss: 0.22711168229579926 at 12:00\n",
            "LSTM - Epoch 10, Loss: 0.1159479171037674 at 12:00\n",
            "LSTM - Epoch 20, Loss: 0.10934529453516006 at 12:00\n",
            "LSTM - Epoch 30, Loss: 0.10644714534282684 at 12:00\n",
            "LSTM - Epoch 40, Loss: 0.10504842549562454 at 12:00\n",
            "LSTM - Epoch 50, Loss: 0.10158321261405945 at 12:00\n",
            "LSTM - Epoch 60, Loss: 0.09195137768983841 at 12:00\n",
            "LSTM - Epoch 70, Loss: 0.05601467937231064 at 12:00\n",
            "LSTM - Epoch 80, Loss: 0.034877240657806396 at 12:00\n",
            "LSTM - Epoch 90, Loss: 0.02484355866909027 at 12:00\n",
            "LSTM - Epoch 100, Loss: 0.018016275018453598 at 12:00\n",
            "LSTM - Epoch 110, Loss: 0.014257851988077164 at 12:00\n",
            "LSTM - Epoch 120, Loss: 0.012540148571133614 at 12:00\n",
            "LSTM - Epoch 130, Loss: 0.011829318478703499 at 12:00\n",
            "LSTM - Epoch 140, Loss: 0.011705302633345127 at 12:00\n",
            "LSTM - Epoch 150, Loss: 0.011623022146522999 at 12:00\n",
            "LSTM - Epoch 160, Loss: 0.011542563326656818 at 12:00\n",
            "Time Slot: 12:00, LSTM R-squared: 0.8897157359866028, RMSE: 0.11118853837251663\n",
            "LSTM - Epoch 0, Loss: 0.23372384905815125 at 14:00\n",
            "LSTM - Epoch 10, Loss: 0.1159312054514885 at 14:00\n",
            "LSTM - Epoch 20, Loss: 0.10843449831008911 at 14:00\n",
            "LSTM - Epoch 30, Loss: 0.10430996119976044 at 14:00\n",
            "LSTM - Epoch 40, Loss: 0.10358815640211105 at 14:00\n",
            "LSTM - Epoch 50, Loss: 0.10262873023748398 at 14:00\n",
            "LSTM - Epoch 60, Loss: 0.1005215272307396 at 14:00\n",
            "LSTM - Epoch 70, Loss: 0.09551819413900375 at 14:00\n",
            "LSTM - Epoch 80, Loss: 0.07988572865724564 at 14:00\n",
            "LSTM - Epoch 90, Loss: 0.03466816619038582 at 14:00\n",
            "LSTM - Epoch 100, Loss: 0.02776980586349964 at 14:00\n",
            "LSTM - Epoch 110, Loss: 0.022326789796352386 at 14:00\n",
            "LSTM - Epoch 120, Loss: 0.017293643206357956 at 14:00\n",
            "LSTM - Epoch 130, Loss: 0.014406620524823666 at 14:00\n",
            "LSTM - Epoch 140, Loss: 0.012998315505683422 at 14:00\n",
            "LSTM - Epoch 150, Loss: 0.012306297197937965 at 14:00\n",
            "LSTM - Epoch 160, Loss: 0.0119917131960392 at 14:00\n",
            "Time Slot: 14:00, LSTM R-squared: 0.8888563854561252, RMSE: 0.10511482506990433\n",
            "LSTM - Epoch 0, Loss: 0.36931273341178894 at 16:00\n",
            "LSTM - Epoch 10, Loss: 0.11924497783184052 at 16:00\n",
            "LSTM - Epoch 20, Loss: 0.10783989727497101 at 16:00\n",
            "LSTM - Epoch 30, Loss: 0.10696990042924881 at 16:00\n",
            "LSTM - Epoch 40, Loss: 0.10637356340885162 at 16:00\n",
            "LSTM - Epoch 50, Loss: 0.1051768884062767 at 16:00\n",
            "LSTM - Epoch 60, Loss: 0.10391958802938461 at 16:00\n",
            "LSTM - Epoch 70, Loss: 0.10241194069385529 at 16:00\n",
            "LSTM - Epoch 80, Loss: 0.09982840716838837 at 16:00\n",
            "LSTM - Epoch 90, Loss: 0.09435471147298813 at 16:00\n",
            "LSTM - Epoch 100, Loss: 0.07975666224956512 at 16:00\n",
            "LSTM - Epoch 110, Loss: 0.039376866072416306 at 16:00\n",
            "LSTM - Epoch 120, Loss: 0.032734330743551254 at 16:00\n",
            "LSTM - Epoch 130, Loss: 0.026004210114479065 at 16:00\n",
            "LSTM - Epoch 140, Loss: 0.02012612670660019 at 16:00\n",
            "LSTM - Epoch 150, Loss: 0.015465320087969303 at 16:00\n",
            "LSTM - Epoch 160, Loss: 0.012415561825037003 at 16:00\n",
            "Time Slot: 16:00, LSTM R-squared: 0.850095671480827, RMSE: 0.11506713181734085\n",
            "LSTM - Epoch 0, Loss: 0.2598831057548523 at 18:00\n",
            "LSTM - Epoch 10, Loss: 0.10296197980642319 at 18:00\n",
            "LSTM - Epoch 20, Loss: 0.10192953795194626 at 18:00\n",
            "LSTM - Epoch 30, Loss: 0.1004408448934555 at 18:00\n",
            "LSTM - Epoch 40, Loss: 0.09964286535978317 at 18:00\n",
            "LSTM - Epoch 50, Loss: 0.09791720658540726 at 18:00\n",
            "LSTM - Epoch 60, Loss: 0.09463363885879517 at 18:00\n",
            "LSTM - Epoch 70, Loss: 0.08658602833747864 at 18:00\n",
            "LSTM - Epoch 80, Loss: 0.06066710501909256 at 18:00\n",
            "LSTM - Epoch 90, Loss: 0.03511074185371399 at 18:00\n",
            "LSTM - Epoch 100, Loss: 0.023029150441288948 at 18:00\n",
            "LSTM - Epoch 110, Loss: 0.01668110303580761 at 18:00\n",
            "LSTM - Epoch 120, Loss: 0.012982402928173542 at 18:00\n",
            "LSTM - Epoch 130, Loss: 0.011670059524476528 at 18:00\n",
            "LSTM - Epoch 140, Loss: 0.011346610262989998 at 18:00\n",
            "LSTM - Epoch 150, Loss: 0.01116866059601307 at 18:00\n",
            "LSTM - Epoch 160, Loss: 0.010990568436682224 at 18:00\n",
            "Time Slot: 18:00, LSTM R-squared: 0.8877896271015804, RMSE: 0.11034467816352844\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# LSTM model class\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# train and eva\n",
        "def train_evaluate_model(X_train, X_test, y_train, y_test, time_slot):\n",
        "    lstm_model = LSTMModel(input_dim=X_train.shape[-1], hidden_dim=64, num_layers=3, output_dim=1)\n",
        "    lstm_criterion = nn.MSELoss()\n",
        "    lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "    lstm_num_epochs = 170\n",
        "    for epoch in range(lstm_num_epochs):\n",
        "        lstm_model.train()\n",
        "        lstm_optimizer.zero_grad()\n",
        "        lstm_output = lstm_model(X_train)\n",
        "        lstm_loss = lstm_criterion(lstm_output, y_train)\n",
        "        lstm_loss.backward()\n",
        "        lstm_optimizer.step()\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'LSTM - Epoch {epoch}, Loss: {lstm_loss.item()} at {time_slot}')\n",
        "\n",
        "    lstm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        lstm_predictions = lstm_model(X_test)\n",
        "        lstm_r_squared = r2_score(y_test.numpy(), lstm_predictions.numpy())\n",
        "        lstm_rmse = np.sqrt(mean_squared_error(y_test.numpy(), lstm_predictions.numpy()))\n",
        "\n",
        "        print(f'Time Slot: {time_slot}, LSTM R-squared: {lstm_r_squared}, RMSE: {lstm_rmse}')\n",
        "\n",
        "location = 'Visitors in Taj Mahal'\n",
        "time_slots = ['8:00', '10:00', '12:00', '14:00', '16:00', '18:00']\n",
        "for time_slot in time_slots:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(*sequence_data_all[(location, time_slot)], test_size=0.2, random_state=42)\n",
        "    train_evaluate_model(X_train, X_test, y_train, y_test, time_slot)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
